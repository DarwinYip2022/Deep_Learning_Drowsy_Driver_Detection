{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "CLASSES = 2\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Image_path     label_x     label_y  \\\n",
      "0  frame_0149_jpg.rf.ad848af69b7adc7a4d67f8d82b92...      0.6625  0.15390625   \n",
      "1  frame_0161_jpg.rf.a7d9f87c6556f2d980a350eb857d...  0.72265625    0.190625   \n",
      "2  frame_0077_jpg.rf.33fd47b720cd708a804198f25d15...  0.40078125  0.34765625   \n",
      "3  frame_0063_jpg.rf.4624fa9ce940d4a3064911fcb3ce...  0.42578125      0.3375   \n",
      "4  frame_0175_jpg.rf.c07c9544bfd1df4e226182f80058...  0.77109375  0.24296875   \n",
      "\n",
      "      label_w     label_h class  \n",
      "0    0.246875   0.2265625     1  \n",
      "1       0.225  0.24609375     1  \n",
      "2  0.47578125   0.0984375     0  \n",
      "3   0.4109375   0.1609375     0  \n",
      "4     0.29375   0.3109375     1  \n",
      "                                            Image_path     label_x  \\\n",
      "767  frame_0077_jpg.rf.3a66aface3c9af519f2ca0fd1a0e...    0.478125   \n",
      "70   frame_0061_jpg.rf.cfa87e213bfb60d9a8c9327dca5b...   0.4078125   \n",
      "855  frame_0028_jpg.rf.19e32f8b894307391def3413c3d1...  0.81484375   \n",
      "485  frame_0191_jpg.rf.e7f6ac183ee8ac8ce7bb666ca19c...  0.53515625   \n",
      "644  frame_0049_jpg.rf.f0c492fa6eb8522e02484a207d7f...    0.503125   \n",
      "\n",
      "        label_y     label_w     label_h class  \n",
      "767  0.52578125    0.415625    0.096875     0  \n",
      "70    0.3453125    0.453125     0.11875     0  \n",
      "855    0.365625   0.1421875   0.0640625     1  \n",
      "485  0.35703125  0.15078125  0.14921875     1  \n",
      "644       0.525   0.4453125   0.1046875     1  \n",
      "                                          Image_path     label_x     label_y  \\\n",
      "0  frame_0001_jpg.rf.8b7a3fd43952e6fb49d005af8ab5...     0.61875  0.25546875   \n",
      "1  frame_0101_jpg.rf.e9950d83f267efea842042bb5d08...  0.40703125   0.3328125   \n",
      "2  frame_0110_jpg.rf.19798f26be9e970155fcfd9e0056...      0.3875  0.32109375   \n",
      "3  frame_0201_jpg.rf.2e37c2b0f354d27a1acff8cfa800...  0.76484375  0.24609375   \n",
      "4  frame_0203_jpg.rf.1152e2258504052bd774a4f22731...  0.78203125   0.2234375   \n",
      "\n",
      "      label_w     label_h class  \n",
      "0  0.21484375   0.1234375     0  \n",
      "1  0.46015625    0.134375     0  \n",
      "2  0.45546875   0.1234375     0  \n",
      "3  0.16015625  0.31015625     1  \n",
      "4   0.2453125  0.30859375     1  \n"
     ]
    }
   ],
   "source": [
    "def read_label_file(label_file):\n",
    "    data = []\n",
    "    with open(label_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Split each line by space (assuming space-separated values)\n",
    "            parts = line.strip().split()\n",
    "            # Extract relevant information from the line\n",
    "            image_path = parts[0]  # Assuming the image path is the first item\n",
    "            # Assuming the rest of the items in the line are label values\n",
    "            labels = parts[1:]\n",
    "            # Append the extracted information as a tuple to the data list\n",
    "            data.append((image_path, *labels))\n",
    "    # Convert the data list to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)  \n",
    "    return df\n",
    "\n",
    "# Read Train Labels\n",
    "label_file = '../../Drowsey_Driver_DL_Data/combined_classification_dataset/combined_train_labels.txt'\n",
    "train_df = read_label_file(label_file)\n",
    "train_df.drop(train_df.columns[1], axis=1, inplace=True)\n",
    "train_df.columns = [\"Image_path\", 'label_x', 'label_y', 'label_w', 'label_h', 'class']\n",
    "print(train_df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "# Split the combined train dataframe into train and validation sets\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(val_df.head())\n",
    "\n",
    "# Read Test Labels\n",
    "label_file = '../../Drowsey_Driver_DL_Data/combined_classification_dataset/combined_test_labels.txt'\n",
    "test_df = read_label_file(label_file)\n",
    "test_df.drop(test_df.columns[1], axis=1, inplace=True)\n",
    "test_df.columns = [\"Image_path\", 'label_x', 'label_y', 'label_w', 'label_h', 'class']\n",
    "print(test_df.head())  # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_path</th>\n",
       "      <th>label_x</th>\n",
       "      <th>label_y</th>\n",
       "      <th>label_w</th>\n",
       "      <th>label_h</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>frame_0121_jpg.rf.f37f9a1d112f9f10da4c183f238f...</td>\n",
       "      <td>0.46796875</td>\n",
       "      <td>0.52265625</td>\n",
       "      <td>0.42890625</td>\n",
       "      <td>0.0734375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>frame_0014_jpg.rf.37dae33a90d67e8d9641deac08e6...</td>\n",
       "      <td>0.47890625</td>\n",
       "      <td>0.51953125</td>\n",
       "      <td>0.4046875</td>\n",
       "      <td>0.1140625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>frame_0220_jpg.rf.03c2eee10097776806a914cd5bed...</td>\n",
       "      <td>0.54453125</td>\n",
       "      <td>0.27265625</td>\n",
       "      <td>0.1328125</td>\n",
       "      <td>0.16875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>frame_0225_jpg.rf.8ce65e178d0c585838cb57a21dc4...</td>\n",
       "      <td>0.49140625</td>\n",
       "      <td>0.26484375</td>\n",
       "      <td>0.1734375</td>\n",
       "      <td>0.196875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>frame_0161_jpg.rf.2f8c007fe4c291caa38e6a95b4cb...</td>\n",
       "      <td>0.75390625</td>\n",
       "      <td>0.215625</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>frame_0029_jpg.rf.1e09b74824f6ebb718e711a0a03b...</td>\n",
       "      <td>0.6203125</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.2703125</td>\n",
       "      <td>0.184375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>frame_0188_jpg.rf.5a8cbde50aa511468281ffa2edfa...</td>\n",
       "      <td>0.71328125</td>\n",
       "      <td>0.18984375</td>\n",
       "      <td>0.2265625</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>frame_0022_jpg.rf.a666ebdc0b31104a8ec23adb88da...</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.27890625</td>\n",
       "      <td>0.3265625</td>\n",
       "      <td>0.159375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>frame_0194_jpg.rf.d8ea56d7856bacab11616a91a414...</td>\n",
       "      <td>0.53828125</td>\n",
       "      <td>0.353125</td>\n",
       "      <td>0.14453125</td>\n",
       "      <td>0.1703125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>frame_0003_jpg.rf.be6261ba0ce6d0ea92edc324912d...</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.2578125</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.1734375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>874 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Image_path     label_x  \\\n",
       "825  frame_0121_jpg.rf.f37f9a1d112f9f10da4c183f238f...  0.46796875   \n",
       "621  frame_0014_jpg.rf.37dae33a90d67e8d9641deac08e6...  0.47890625   \n",
       "529  frame_0220_jpg.rf.03c2eee10097776806a914cd5bed...  0.54453125   \n",
       "398  frame_0225_jpg.rf.8ce65e178d0c585838cb57a21dc4...  0.49140625   \n",
       "235  frame_0161_jpg.rf.2f8c007fe4c291caa38e6a95b4cb...  0.75390625   \n",
       "..                                                 ...         ...   \n",
       "106  frame_0029_jpg.rf.1e09b74824f6ebb718e711a0a03b...   0.6203125   \n",
       "270  frame_0188_jpg.rf.5a8cbde50aa511468281ffa2edfa...  0.71328125   \n",
       "860  frame_0022_jpg.rf.a666ebdc0b31104a8ec23adb88da...    0.521875   \n",
       "435  frame_0194_jpg.rf.d8ea56d7856bacab11616a91a414...  0.53828125   \n",
       "102  frame_0003_jpg.rf.be6261ba0ce6d0ea92edc324912d...      0.6625   \n",
       "\n",
       "        label_y     label_w    label_h class  \n",
       "825  0.52265625  0.42890625  0.0734375     0  \n",
       "621  0.51953125   0.4046875  0.1140625     0  \n",
       "529  0.27265625   0.1328125    0.16875     0  \n",
       "398  0.26484375   0.1734375   0.196875     0  \n",
       "235    0.215625        0.25    0.26875     1  \n",
       "..          ...         ...        ...   ...  \n",
       "106    0.234375   0.2703125   0.184375     0  \n",
       "270  0.18984375   0.2265625   0.265625     1  \n",
       "860  0.27890625   0.3265625   0.159375     0  \n",
       "435    0.353125  0.14453125  0.1703125     1  \n",
       "102   0.2578125      0.2875  0.1734375     0  \n",
       "\n",
       "[874 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_image_and_label_train(image_name, label_x, label_y, label_w, label_h, class_label, target_size=(244, 244)):\n",
    "    # Construct the full image path\n",
    "    image_path = os.path.join('/Users/sameeraboppana/Desktop/DL_Project/combined_classification_dataset/train/images', image_name)\n",
    "    \n",
    "    # Read and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image from {image_path}\")\n",
    "        return None, None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = image.shape\n",
    "    x, y, w, h = int(float(label_x) * width), int(float(label_y) * height), int(float(label_w) * width), int(float(label_h) * height)\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "    cropped_image = cv2.resize(cropped_image, target_size)\n",
    "    \n",
    "    # Preprocess the class label\n",
    "    class_label = int(class_label)  # Convert to integer\n",
    "    \n",
    "    return cropped_image, class_label\n",
    "\n",
    "# Lists to store preprocessed images and their corresponding class labels\n",
    "train_preprocessed_images = []\n",
    "train_class_labels = []\n",
    "\n",
    "# Iterate through each row in train_df and preprocess the images and labels\n",
    "for index, row in train_df.iterrows():\n",
    "    image_name = row['Image_path']  # Assuming this contains just the image filename\n",
    "    label_x = row['label_x']\n",
    "    label_y = row['label_y']\n",
    "    label_w = row['label_w']\n",
    "    label_h = row['label_h']\n",
    "    class_label = row['class']\n",
    "\n",
    "    # Preprocess the image and label\n",
    "    image, class_label = preprocess_image_and_label_train(image_name, label_x, label_y, label_w, label_h, class_label)\n",
    "    \n",
    "    # Append preprocessed image and class label to lists\n",
    "    train_preprocessed_images.append(image)\n",
    "    train_class_labels.append(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (786, 244, 244, 3)\n",
      "Train labels shape: (786,)\n",
      "Validation images shape: (88, 244, 244, 3)\n",
      "Validation labels shape: (88,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_preprocessed_images, train_class_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Optionally, convert the lists to numpy arrays for compatibility with TensorFlow/Keras\n",
    "import numpy as np\n",
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# Print the shapes of the train and validation sets\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 15s]\n",
      "val_accuracy: 0.5795454382896423\n",
      "\n",
      "Best val_accuracy So Far: 0.9431818127632141\n",
      "Total elapsed time: 00h 18m 10s\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Define your CNN model architecture\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers with varying number of filters\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        input_shape=(244, 244, 3),  # Adjust input shape according to your preprocessed image size\n",
    "        kernel_regularizer=regularizers.l2(0.001)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('conv_2_filter', min_value=64, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.001)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('conv_3_filter', min_value=128, max_value=256, step=32),\n",
    "        kernel_size=hp.Choice('conv_3_kernel', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.001)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=32),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.001)\n",
    "    ))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust if needed\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize Keras Tuner RandomSearch\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of hyperparameter combinations to try\n",
    "    directory='my_dir',\n",
    "    project_name='random_search'\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed_images = []\n",
    "test_class_labels = []\n",
    "\n",
    "def preprocess_image_and_label_test(image_name, label_x, label_y, label_w, label_h, class_label, target_size=(244, 244)):\n",
    "    # Construct the full image path\n",
    "    image_path = os.path.join('/Users/sameeraboppana/Desktop/DL_Project/combined_classification_dataset/test/images', image_name)\n",
    "    \n",
    "    # Read and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image from {image_path}\")\n",
    "        return None, None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = image.shape\n",
    "    x, y, w, h = int(float(label_x) * width), int(float(label_y) * height), int(float(label_w) * width), int(float(label_h) * height)\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "    cropped_image = cv2.resize(cropped_image, target_size)\n",
    "    \n",
    "    # Preprocess the class label\n",
    "    class_label = int(class_label)  # Convert to integer\n",
    "    \n",
    "    return cropped_image, class_label\n",
    "\n",
    "\n",
    "# Iterate through each row in test_df and preprocess the images and labels\n",
    "for index, row in test_df.iterrows():\n",
    "    image_name = row['Image_path']  # Assuming this contains just the image filename\n",
    "    label_x = row['label_x']\n",
    "    label_y = row['label_y']\n",
    "    label_w = row['label_w']\n",
    "    label_h = row['label_h']\n",
    "    class_label = row['class']\n",
    "\n",
    "    # Preprocess the image and label\n",
    "    image, class_label = preprocess_image_and_label_test(image_name, label_x, label_y, label_w, label_h, class_label)\n",
    "    \n",
    "    # Append preprocessed image and class label to lists\n",
    "    test_preprocessed_images.append(image)\n",
    "    test_class_labels.append(class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 494ms/step - accuracy: 0.7221 - loss: 2.1068 - val_accuracy: 0.6364 - val_loss: 2.2237\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 477ms/step - accuracy: 0.7366 - loss: 0.8845 - val_accuracy: 0.8409 - val_loss: 0.5912\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 467ms/step - accuracy: 0.8000 - loss: 0.9185 - val_accuracy: 0.7386 - val_loss: 0.6779\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 844ms/step - accuracy: 0.7773 - loss: 0.6957 - val_accuracy: 0.7273 - val_loss: 0.7257\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 467ms/step - accuracy: 0.7633 - loss: 0.8646 - val_accuracy: 0.7614 - val_loss: 0.7202\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 473ms/step - accuracy: 0.7694 - loss: 0.8430 - val_accuracy: 0.9091 - val_loss: 0.4550\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 779ms/step - accuracy: 0.8404 - loss: 0.5625 - val_accuracy: 0.9091 - val_loss: 0.5032\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 491ms/step - accuracy: 0.8565 - loss: 0.5327 - val_accuracy: 0.8864 - val_loss: 0.4982\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 485ms/step - accuracy: 0.8530 - loss: 0.4919 - val_accuracy: 0.9205 - val_loss: 0.4528\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 475ms/step - accuracy: 0.8675 - loss: 0.4478 - val_accuracy: 0.8523 - val_loss: 0.5037\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8624 - loss: 0.4302\n",
      "Test Loss: 0.41673749685287476\n",
      "Test Accuracy: 0.8717948794364929\n"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model on the combined train and validation data\n",
    "best_model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n",
    "\n",
    "# Convert test_preprocessed_images and test_class_labels to numpy arrays\n",
    "test_images = np.array(test_preprocessed_images)\n",
    "test_labels = np.array(test_class_labels)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "Actual Label: 0 Predicted Label: [0]\n",
      "Actual Label: 0 Predicted Label: [0]\n",
      "Actual Label: 0 Predicted Label: [0]\n",
      "Actual Label: 1 Predicted Label: [1]\n",
      "Actual Label: 1 Predicted Label: [1]\n",
      "Actual Label: 0 Predicted Label: [0]\n",
      "Actual Label: 1 Predicted Label: [0]\n",
      "Actual Label: 1 Predicted Label: [0]\n",
      "Actual Label: 0 Predicted Label: [0]\n",
      "Actual Label: 0 Predicted Label: [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = best_model.predict(test_images)\n",
    "\n",
    "# Convert predictions to class labels (assuming binary classification)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Display some predictions\n",
    "for i in range(10):  # Displaying predictions for the first 10 samples\n",
    "    print(\"Actual Label:\", test_labels[i], \"Predicted Label:\", predicted_labels[i])\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "def plot_confusion_matrix(cm):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(test_labels))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Predicted label for the new image: 0\n"
     ]
    }
   ],
   "source": [
    "new_image_path = \"/Users/sameeraboppana/Desktop/DL_Project/PHOTO-2024-05-29-19-11-53.jpg\"\n",
    "\n",
    "# Read the new image\n",
    "new_image = cv2.imread(new_image_path)\n",
    "if new_image is None:\n",
    "    print(f\"Error: Unable to load image from {new_image_path}\")\n",
    "else:\n",
    "    # Preprocess the new image (resize, normalize, etc.)\n",
    "    preprocessed_new_image = cv2.resize(new_image, (244, 244))  # Resize to match the input size of your model\n",
    "    \n",
    "    # Make prediction on the preprocessed new image\n",
    "    prediction = model.predict(np.expand_dims(preprocessed_new_image, axis=0))\n",
    "    \n",
    "    # Convert prediction to class label\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    \n",
    "    print(\"Predicted label for the new image:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
